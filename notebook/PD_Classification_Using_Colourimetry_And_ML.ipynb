{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "176c04e2-d277-4e39-9319-c4ae8f4ae243",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This workbook explains the core priciples of the colourimetric based segmentation of microscope images, and ends with the classification of PD using segmentations of slide imagery.  It loads a number of basis functions for the Aperio L2 Microscope sensor, brain and staining functions, and processes them to produce likely RGB values found within the real imagery.  These RGB values in the real images are then estimated for their basis componenets by fitting them to the likely RGB values.  Likely candidate regions, those containing an amount of DAB stain, are segmented and saved as JPG files that can then be passed to a CNN developed previously (PDNet).\n",
    "\n",
    "At the end of this workbook we will have a classifier that can efficiently and effectively detect PD pathology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea473d8e-1ecc-4192-84d2-d74f8872049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory so 'Polygeist' is in the path.\n",
    "import os\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "import pathlib\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "\n",
    "# OS & Utilities\n",
    "from glob import glob\n",
    "\n",
    "# Maths, Image, Science Libraries\n",
    "import imageio.v3 as io\n",
    "import numpy as np\n",
    "from pqdm.processes import pqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "# Polygeist libraries\n",
    "import polygeist.colour as pc\n",
    "from polygeist.slidecore.slide import (\n",
    "    AperioSlide,\n",
    "    SpectralSlideGenerator,\n",
    "    SyntheticSlide,\n",
    ")\n",
    "from polygeist.training import train_model\n",
    "from polygeist.utils import (\n",
    "    calc_median_score_list,\n",
    "    load_filenames_and_generate_conditions,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb76a5a-1774-40e1-ae67-621282dd0ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loads matplotlib, a plotting library with the ability to display scalable plots in the browser.\n",
    "%matplotlib widget\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eae51e-dc0d-4aa4-99f5-83705a6f76ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will display imagery relevant to the tutorial using this function, the files will be in a folder called 'assets'\n",
    "def show_explainer_image(filename, title):\n",
    "    im = io.imread(filename)\n",
    "    plt.figure()\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(im)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe491d0-36b5-4089-808d-5fe57f415c59",
   "metadata": {},
   "source": [
    "# Photometric Calibration of the microscope\n",
    "\n",
    "To effectively find the target protein (here a-syn) we need to understand how it appears to the microscope that captured the sample.  We can do this by determining the sensor response from our microscope, and then, given the integration of our stain transmission spectra, our lightsource, and our sensor sensitivity functions, we can determine the colour of the protein stained pixels.\n",
    "\n",
    "Here we:\n",
    "* Load the sensitivities of the sensor, the illumination power of the lightsource (by wl) and the transmission of the stains / matter under examination.\n",
    "* Calculate the sensor response array, to be used to decompose a sensor response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a1fc6-c767-46f1-92b0-a800407230ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_explainer_image(\n",
    "    \"notebook/assets/spectral_modelling.png\",\n",
    "    \"Spectral Transmission Functions to Sensor RGB Response\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f529a847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polygeist.microscope import Microscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = Microscope()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc05f080-893e-413f-8d73-5c3812e9ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dab Stain\n",
    "DAB = pc.Illuminant.from_file_with_range(\"spectral/histology/DABL20Vector.csv\", ms.wl)\n",
    "# Hemotoxylin\n",
    "H = pc.Illuminant.from_file_with_range(\"spectral/histology/HemotoxylinC19.csv\", ms.wl)\n",
    "# Eosin\n",
    "E = pc.Illuminant.from_file_with_range(\"spectral/histology//Eosin6um.csv\", ms.wl)\n",
    "# Healthy Brain Absorption\n",
    "_B = pc.Illuminant.from_file_with_range(\n",
    "    \"spectral/histology/HealthyBrainAbsorption9um.csv\", ms.wl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096aaab2-69ae-4131-9c5f-c4421a1fdef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Transmission Function for the brain\n",
    "_B[np.isnan(_B)] = 0\n",
    "B = 1.0 - (_B / np.max(_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b742b896-aacd-4e3a-8f70-26a3ea47facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the array of sensor RGB values for the given spectra\n",
    "responses = np.vstack([ms.response(s) for s in [DAB, H, ms.ls, B, E]])\n",
    "print(responses)\n",
    "\n",
    "# These are sensor specific responses for the brain transmission * filter responses,\n",
    "# for the AT2 sensor and lightsource.\n",
    "# See reports for how to generate them for other sensing systems.\n",
    "\n",
    "# 7.10357511 12.61506218 13.59695489  # DAB\n",
    "# 8.81961536 10.18302502  5.08567669  # Hematoxylin\n",
    "# 11.02074647 15.41804066 12.77042165 # Light Source\n",
    "# 17.05035857 17.64819458  9.17788779 # Brain Transmission\n",
    "# 0.45574971  4.32897163  1.68161384  # Eosin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce591078-15b8-4208-bc73-72988dbd174b",
   "metadata": {},
   "source": [
    "## Sensor Response and Example Fit\n",
    "\n",
    "The array above shows sensor BGR responses of the microscope sensor given the maximum brightness from the microscope lightsource and that surface in isolation.  Now we are going to use these values in an example spectral fit.  We will load an example slide file (or use simulated data), and find pixels that contain DAB stain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7cd75-c772-4243-ada6-85f0683f767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_explainer_image(\n",
    "    \"notebook/assets/spectral_fitting.png\",\n",
    "    \"Process of Decomposing a Pixel into Spectral Weights\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386c9fee-7165-4780-8ba3-cb90186f01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data or use simulated data\n",
    "simulated_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f979a467-2e05-4823-b9da-35827415047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your path here if you are not using simulated data!\n",
    "example_slide_path = \"localnas/A-syn cases/PD/PD788/PD788-17_A-syn.svs\"\n",
    "\n",
    "if simulated_data:\n",
    "    image = SpectralSlideGenerator(width=100, height=100).image\n",
    "else:\n",
    "    image = AperioSlide(example_slide_path).get_slide_with_pixel_resolution_in_microns(\n",
    "        2.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2a5d34-c3b8-43db-9bfe-d70b695be0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(image.astype(int))\n",
    "plt.title(\"Slide Image Containing Pixel Data of Tissue and Stain\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe1044-b1ee-417c-9037-fbd41bfad15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the normalisation function we will use to scale within our planes\n",
    "def normalise(F):\n",
    "    F += max(np.abs(F.min()), np.abs(F.max()))\n",
    "    F /= F.max()\n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0fa2a1-ae82-4eab-b2ab-3d2f6e023f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our image to Nx3 Tristimulus values\n",
    "T_I = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "\n",
    "# Least squares fit to our response functions\n",
    "T_x = np.linalg.lstsq(responses.T, T_I.T, rcond=None)\n",
    "\n",
    "# Normalise our responses per map for DAB and Brain Transmission\n",
    "DAB = normalise(T_x[0][0].copy())\n",
    "BT = normalise(T_x[0][3].copy())\n",
    "\n",
    "raw_DAB = DAB - BT\n",
    "raw_DAB = raw_DAB.reshape(image.shape[0], image.shape[1])\n",
    "\n",
    "_DAB = T_x[0][0].reshape(image.shape[0], image.shape[1])\n",
    "_BT = T_x[0][3].reshape(image.shape[0], image.shape[1])\n",
    "\n",
    "# Values below our raw threshold will be considered A-syn (typically BT has significantly more power)\n",
    "raw_threshold = -0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224d3d3d-b858-424e-bb26-d8afdb33c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show our decomposition\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "\n",
    "fig.suptitle(\"Comparisons between Original, Decomposed & Isolated Signals\")\n",
    "axs[0, 0].set_title(\"RGB Image\")\n",
    "axs[0, 0].imshow(image.astype(int))\n",
    "axs[0, 1].set_title(\"DAB Estimate\")\n",
    "axs[0, 1].imshow(normalise(_DAB))\n",
    "axs[1, 0].set_title(\"Brain Transmission Estimate\")\n",
    "axs[1, 0].imshow(normalise(_BT))\n",
    "axs[1, 1].set_title(\"Asyn Isolation\")\n",
    "# Here we threshold the raw DAb signal to get A-syn\n",
    "axs[1, 1].imshow(raw_DAB < raw_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e665640-ee6d-4f01-80e5-7e80cfab6c39",
   "metadata": {},
   "source": [
    "# Example Segmentation of Images Using the Microscope Calibration\n",
    "\n",
    "Above you can see the decomposed channels for each of the proteins, and the asyn isolation channel is a boolean mask, where we have marked each pixel that exceeds a threshold and highlighted it as a DAB pixel.\n",
    "\n",
    "Now we can use this technology in a segmentation routine, here we will traverse our slides (or simulated data) and with a given window size, we will segment those windows into the same boolean channel as can be seen above in 'Asyn Isolation'.  We will then check that window to see what the mean activation (density of pixels are 'on' (yellow above)) and then write our regions to disk that exceed that threshold.  We will also save the density information into a 'density map' for later use.\n",
    "\n",
    "We will do this for both the PD and Control groups, and then use those segmentation to train a classifier, as the control group should be exclusively false alarms.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a9d3c6-80f6-48f2-9e3c-d9b9161a1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_explainer_image(\n",
    "    \"notebook/assets/density_map_generation.png\",\n",
    "    \"Turning Pathology Slides into Density Maps and ROIs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb712f9-26a2-4fad-9c77-5e88bd27d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # The threshold level in RGB to use for our prexisting RGB thresholding routines from Phase 1\n",
    "    \"B_channel_threshold\": 20,\n",
    "    # This is where we will be storing our segmeneted data\n",
    "    \"dump_path\": \"/run/media/brad/ScratchM2/maps_dump_512_jpeg/\",\n",
    "    # This is our window size, over which we will iterate.\n",
    "    \"map_stride\": 512,\n",
    "    # Our negative cases, our controls, are stored in the 'Controls' folder\n",
    "    \"negative_case_folder\": \"Controls\",\n",
    "    # Our positive, pd cases, are stored in the 'PD' folder\n",
    "    \"positive_case_folder\": \"PD\",\n",
    "    # Other search paths to look for cases\n",
    "    \"search_directories\": [\n",
    "        \"/run/media/brad/TBSSD1/A-syn cases/\",\n",
    "        \"/run/media/brad/TBSSD2/A-syn cases/\",\n",
    "    ],\n",
    "    # These are our case filenames\n",
    "    \"case_files\": \"Data/filenames/asyn_files.txt\",\n",
    "    # For use with real data, this filters slides with the wrong index\n",
    "    \"slide_index_filter\": \"17_A\",\n",
    "    # Toggle this is you are using simulated data\n",
    "    \"simulated\": simulated_data,\n",
    "    # Number to simulate\n",
    "    \"simulation_n\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a34e84c-fc97-4d68-a191-58d64152daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_decompose_and_dump_jpeg(\n",
    "    slice,\n",
    "    stride=1000,\n",
    "    process_raw=True,\n",
    "    threshold=0.175,\n",
    "    pc=0.0375,\n",
    "    raw_threshold=-0.3,\n",
    "    raw_pc=0.00125,\n",
    "    jpeg_dump_path_and_name=\"start_name\",\n",
    "):\n",
    "\n",
    "    # These are sensor specific responses for the brain transmission * filter responses, for the AT2 sensor\n",
    "    # and lightsource.  See reports for how to generate them for other sensing systems\n",
    "    responses = np.array(\n",
    "        np.matrix(\n",
    "            \"[ 7.10357511 12.61506218 13.59695489 ; \"  # DAB\n",
    "            \"8.81961536 10.18302502  5.08567669; \"  # Hematoxylin\n",
    "            \"11.02074647 15.41804066 12.77042165 ; \"  # Light Source\n",
    "            \"17.05035857 17.64819458  9.17788779; \"  # Brain Transmission\n",
    "            \"0.45574971  4.32897163  1.68161384]\"\n",
    "        )\n",
    "    )  # Eosin\n",
    "\n",
    "    # Get slide at native resolution\n",
    "    image = slice.get_slide_with_pixel_resolution_in_microns(2.0)\n",
    "\n",
    "    # Get the height and width of the slice\n",
    "    yy, xx, _ = image.shape\n",
    "\n",
    "    # Create a densities array to store the local densities\n",
    "    x_pass = int(np.ceil(xx / stride))\n",
    "    y_pass = int(np.ceil(yy / stride))\n",
    "    densities = np.zeros((y_pass, x_pass))\n",
    "\n",
    "    # Convert our image to Nx3 Tristimulus values\n",
    "    T_I = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "\n",
    "    # Least squares fit to our response functions\n",
    "    T_x = np.linalg.lstsq(responses.T, T_I.T, rcond=None)\n",
    "\n",
    "    # Normalise our reponses per map for DAB and Brain Transmission\n",
    "    DAB = normalise(T_x[0][0].copy())\n",
    "    BT = normalise(T_x[0][3].copy())\n",
    "\n",
    "    # Remove the background (Brain) fom DAB response, leaving just the dab mask\n",
    "    # Remove the background (Brain) fom DAB response, leaving just the dab mask\n",
    "    raw_DAB = DAB - BT\n",
    "    raw_DAB = raw_DAB.reshape(image.shape[0], image.shape[1])\n",
    "\n",
    "    # ticker for JPEG index\n",
    "    dump_number = 0\n",
    "    if not process_raw:\n",
    "        # Tumble over the slice using a fixed window size\n",
    "        for xi, x in enumerate(np.arange(0, xx, stride)):\n",
    "            for yi, y in enumerate(np.arange(0, yy, stride)):\n",
    "                # Grab this section x -> x + stride, y -> y + stride\n",
    "                section = raw_DAB[y : y + stride, x : x + stride]\n",
    "\n",
    "                diff = np.abs(\n",
    "                    section[:, :, 0].astype(float) - section[:, :, 2].astype(float)\n",
    "                )\n",
    "                if np.sum(diff > threshold) / (stride**2) > pc:\n",
    "                    io.imwrite(\n",
    "                        f\"{jpeg_dump_path_and_name}{dump_number}.jpg\",\n",
    "                        image[y : y + stride, x : x + stride, :],\n",
    "                    )\n",
    "                    dump_number += 1\n",
    "    else:\n",
    "        # Tumble over the slice using a fixed window size\n",
    "        for xi, x in enumerate(np.arange(0, xx, stride)):\n",
    "            for yi, y in enumerate(np.arange(0, yy, stride)):\n",
    "                # Grab this section x -> x + stride, y -> y + stride\n",
    "                section = raw_DAB[y : y + stride, x : x + stride] < raw_threshold\n",
    "\n",
    "                if np.mean(section) > raw_pc:\n",
    "                    io.imwrite(\n",
    "                        f\"{jpeg_dump_path_and_name}{dump_number}.jpg\",\n",
    "                        image[y : y + stride, x : x + stride, :],\n",
    "                    )\n",
    "                    dump_number += 1\n",
    "    return densities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8910b19e-e380-42e9-bd84-324bd252b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This process function will be called in parallel on each slide.\n",
    "def process(argv):\n",
    "    file = argv[\"file\"]\n",
    "    cnd = argv[\"condition\"]\n",
    "    simulated = argv[\"sim\"]\n",
    "\n",
    "    try:\n",
    "        file_name = os.path.basename(os.path.normpath(file))\n",
    "        slide = AperioSlide(file) if not simulated else SyntheticSlide(file)\n",
    "    except:\n",
    "        return file\n",
    "    d = spectral_decompose_and_dump_jpeg(\n",
    "        slide,\n",
    "        stride=config[\"map_stride\"],\n",
    "        jpeg_dump_path_and_name=f'{config[\"dump_path\"]}/{cnd}/{file_name}',\n",
    "    )\n",
    "    with open(f'{config[\"dump_path\"]}/{cnd}/{file_name}.npy', \"wb\") as f:\n",
    "        np.save(f, d)\n",
    "\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61fdf90-3d87-4ef7-8ba2-da10cb99979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the directory structure to save our files\n",
    "for pth in [config[\"positive_case_folder\"], config[\"negative_case_folder\"]]:\n",
    "    pathlib.Path(config[\"dump_path\"] + pth).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "files_list_and_configuration = []\n",
    "# For each case in positive/negative case directories, or for simulated data\n",
    "if not config[\"simulated\"]:\n",
    "    for cases in [config[\"positive_case_folder\"], config[\"negative_case_folder\"]]:\n",
    "        for searched_path in config[\"search_directories\"]:\n",
    "            for p in glob(searched_path + cases + \"/*/\", recursive=True):\n",
    "                for f in glob(p + \"/*.svs\", recursive=True):\n",
    "                    if config[\"slide_index_filter\"] in f:\n",
    "                        files_list_and_configuration.append(\n",
    "                            {\n",
    "                                \"file\": f,\n",
    "                                \"condition\": cases,\n",
    "                                \"sim\": False,\n",
    "                            }\n",
    "                        )\n",
    "else:\n",
    "    for i in np.arange(0, config[\"simulation_n\"]):\n",
    "        # Which condition to generate, PD or Control\n",
    "        condition = (\n",
    "            config[\"positive_case_folder\"]\n",
    "            if np.random.random() > 0.5\n",
    "            else config[\"negative_case_folder\"]\n",
    "        )\n",
    "        # We will use the dataset name conv for our slides.\n",
    "        name_for_slide = \"PD\" if \"PD\" in condition else \"PDC\"\n",
    "        # Generate a new slide and save it to disk\n",
    "        SpectralSlideGenerator(\n",
    "            512 * 4,\n",
    "            512 * 4,\n",
    "            filename=f\"{config['dump_path']}/{name_for_slide}{i}-17_A.png\",\n",
    "            control=\"C\" in name_for_slide,\n",
    "        )\n",
    "        # Append and continue\n",
    "        files_list_and_configuration.append(\n",
    "            {\n",
    "                \"file\": f\"{config['dump_path']}/{name_for_slide}{i}-17_A.png\",\n",
    "                \"condition\": condition,\n",
    "                \"sim\": True,\n",
    "            }\n",
    "        )\n",
    "\n",
    "result = pqdm(\n",
    "    files_list_and_configuration, process, n_jobs=1\n",
    ")  # 1 job for AJAX optimistion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99794e1f-ef2f-43d6-9f6e-2ba71db870f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_conditions = load_filenames_and_generate_conditions(config[\"case_files\"])\n",
    "\n",
    "positive_cases = []\n",
    "negative_cases = []\n",
    "for case, condition in case_conditions.items():\n",
    "    positive_cases.append(case) if \"C\" not in condition else negative_cases.append(case)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fd27b3-b315-420b-84f7-6cf6bc1b15c5",
   "metadata": {},
   "source": [
    "# Raw Analysis of Density Maps\n",
    "\n",
    "Let's first just take a look at the density maps that have been dumped.  We are going to use the basic thresholding routines to determine if we have successfully segmented asyn, before we look into more advanced classifications methods in the other WPs.\n",
    "\n",
    "Process:\n",
    "\n",
    "* Create histograms of the density for each case, and band pass those densities to highlight the maximal differences between the groups.\n",
    "* Calculate the mean bandpass density for each 'case', and then produce an ROC on the basis of those densities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab750842-9df7-440f-a656-284e432c4775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use a simple median classifier, with a low RGB threshold as a simple mask.\n",
    "# Note this is for illustrative purposes,\n",
    "# we are going to explore a better classification method in the following workbooks.\n",
    "\n",
    "# Count results of simple median classifier\n",
    "positive_score_list = calc_median_score_list(\n",
    "    positive_cases,\n",
    "    f'{config[\"dump_path\"]}/{config[\"positive_case_folder\"]}/*17_*.jpg',\n",
    "    rgb_threshold=config[\"B_channel_threshold\"],\n",
    ")\n",
    "\n",
    "negative_score_list = calc_median_score_list(\n",
    "    negative_cases,\n",
    "    f'{config[\"dump_path\"]}/{config[\"negative_case_folder\"]}/*17_*.jpg',\n",
    "    rgb_threshold=config[\"B_channel_threshold\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb40a8-604d-41de-8c01-e4f0aee3e85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the median Counts\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot([negative_score_list, positive_score_list], showfliers=False)\n",
    "ax.set_xticks([1, 2], [\"Control\", \"PD\"])\n",
    "plt.xlabel(\"Condition\")\n",
    "plt.ylabel(\"Quartiles of ROIs Identified\")\n",
    "plt.title(\"Interquartile Ranges for Each Case's Median Density\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75326b80-118b-475b-a283-a99683c236a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.hstack(\n",
    "    [np.ones(len(positive_score_list)), np.zeros(len(negative_score_list))]\n",
    ")\n",
    "outputs = np.hstack([positive_score_list, negative_score_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba51186-0348-4a63-89ef-bdbc53d774dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(labels, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec03a945-079d-4f25-a942-7aa5132238c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"PD vs Control\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Alarm Rate\", fontsize=18)\n",
    "plt.ylabel(\"Hit Rate\", fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "# plt.title(\"512um Patch Level Discrmination between Taupathology and Control Tau Segmentation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f81aa1-ae61-4d9d-a579-c372c326c54b",
   "metadata": {},
   "source": [
    "# Machine Learning with PDNet\n",
    "\n",
    "Above we can see fair classifcation based on a single statistics, the median density from the ROI maps.  Now we are going to take those segmented ROIs, and use them to train PDNet.  We will:\n",
    "   - Split the ROIs into training and test sets\n",
    "   - Use the training set to train PDNet\n",
    "   - Evaluate the per ROI classification accuracy of PDNet using the confidence scores from the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ba2766-dd7d-4520-8c09-c4af9f774a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_explainer_image(\n",
    "    \"notebook/assets/pdnet_training.png\", \"Schematic of Training PDNet on ROIs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304e1c33-23f2-4490-903b-8cbf89d633f2",
   "metadata": {},
   "source": [
    "## Dataprep\n",
    "Here we ready the folders for our model output, and our training and test data.  For real segmentations, we will balance our sets using randomly sampled real slide windows (non-ROIs).  This will be skipped for simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb5f4d-9044-4b66-a4c0-dcc1c63d99ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dump path for our model training run, model checkpoints will be saved here\n",
    "model_dump_dir = f\"{config['dump_path']}/model_dump\"\n",
    "batch_size = 32  # Adjust for memory constraints (may effect results)\n",
    "num_epochs = 500  # Adjust for time available for training (may effect results)\n",
    "\n",
    "# Note, if training fails even with small memory requirements, you may need to restart the kernel\n",
    "# (The staining model may not have released its memory.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2810c3f3-eb58-4284-a85c-a6d5f262f6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have our folders, we need to create a training and validation set.\n",
    "# We will use a clean copy of the data for performance, repeatability and safety.\n",
    "training_dump_path = f\"{config['dump_path']}/training_dump\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fc7020-4c56-4be6-89a6-c558c5fab89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the directory structure to save our files\n",
    "for pth in [training_dump_path, model_dump_dir]:\n",
    "    pathlib.Path(pth).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc7cdba-09d2-4d3e-a32f-70c1d097d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the directory structure to save our files\n",
    "for pth in [training_dump_path + \"/train/\", training_dump_path + \"/val/\"]:\n",
    "    for s in [\"Controls\", \"PD\"]:\n",
    "        pathlib.Path(pth + s).mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5b6624-2f29-4495-8603-d2b56e4e4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should we copy files to create training dataset?\n",
    "# Skip this cell if the data has already been prepared\n",
    "# Do not run twice as it does not remove old datasets.\n",
    "skip = len(glob(f\"{training_dump_path}/train/Controls/*.jpg\")) > 0\n",
    "\n",
    "if not skip:\n",
    "    # Splits\n",
    "    prop_data_train = 0.6\n",
    "\n",
    "    # Copy and partition the files (train and val)\n",
    "    for s in [\"Controls\", \"PD\"]:\n",
    "        for file in glob(f\"{config['dump_path']}/{s}/*.jpg\"):\n",
    "            # basename for dumping out\n",
    "            base = os.path.basename(file)\n",
    "            f = io.imread(file)\n",
    "            if f.shape[0] == f.shape[1]:\n",
    "                if random.random() > (1.0 - prop_data_train):\n",
    "                    shutil.copyfile(file, f\"{training_dump_path}/train/{s}/\" + base)\n",
    "                else:\n",
    "                    shutil.copyfile(file, f\"{training_dump_path}/val/{s}/\" + base)\n",
    "\n",
    "# Now we are going to balance the training and test sets, to prevent overfitting, we do this by randomly sampling new\n",
    "# regions from the control set images until the number of control squares is the same of the test squares.\n",
    "# !! NOTE !! This only works when set A > set B.  If set B was larger, we would have to get more examples from set A\n",
    "# where a random sample would not work.\n",
    "l_control = len(glob(f\"{training_dump_path}/train/Controls/*.jpg\"))\n",
    "l_test = len(glob(f\"{training_dump_path}/train/PD/*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af0a24-cc65-4a07-bd56-41c67747baf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config[\"simulated\"]:\n",
    "    from polygeist.slidecore import AperioSlide as Slide\n",
    "\n",
    "    # list of files in control list\n",
    "    control_file_list = glob(f\"{config['search_directories'][0]}/Controls/*/*.svs\")\n",
    "\n",
    "    # Continue while set is unbalanced ~100\n",
    "    control_injection_index = 0\n",
    "    while l_test > l_control:\n",
    "        # Randomise the list\n",
    "        random.shuffle(control_file_list)\n",
    "\n",
    "        # Sample the top of the list\n",
    "        slide = Slide(control_file_list[0]).get_slide_with_pixel_resolution_in_microns(\n",
    "            2.0\n",
    "        )\n",
    "        filename = os.path.basename(control_file_list[0])\n",
    "\n",
    "        yy, xx, _ = slide.shape\n",
    "\n",
    "        # Create a densities array to store the local densities\n",
    "        x_pass = int(np.ceil(xx / 512))\n",
    "        y_pass = int(np.ceil(yy / 512))\n",
    "\n",
    "        for x, y in zip(\n",
    "            np.random.randint(0, x_pass - 1, 25), np.random.randint(0, y_pass - 1, 25)\n",
    "        ):\n",
    "            im = slide[\n",
    "                (y * 512) : (y * 512) + 512, (x * 512) : (x * 512) + 512, :\n",
    "            ].copy()\n",
    "            io.imwrite(\n",
    "                f\"{training_dump_path}/train/Controls/CI_{filename}_{control_injection_index}.jpg\",\n",
    "                im,\n",
    "            )\n",
    "            control_injection_index += 1\n",
    "\n",
    "        # Recount\n",
    "        l_control = len(glob(f\"{training_dump_path}/train/Controls/*.jpg\"))\n",
    "        l_test = len(glob(f\"{training_dump_path}/train/PD/*.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8503183-00c3-40d1-bb06-c80991f4c85b",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now that we have our training set in place, we can call our `train_model` function which will handle the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c099c909-d651-4486-b1b3-5ecd55d54873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't inject into the validation set, that is kept clean for validation of the colourimetric segmentation.\n",
    "\n",
    "# Start a timer\n",
    "start_time = time.time()\n",
    "\n",
    "latest_model_name = train_model(\n",
    "    training_dump_path, model_dump_dir, batch_size, num_epochs, strict=False\n",
    ")\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "print(f\"Training complete in {time_elapsed // 60}m {time_elapsed % 60}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c772fd17-e660-45d0-b068-960e460052cd",
   "metadata": {},
   "source": [
    "## Validation\n",
    "Here, we use the output of the model, which is stored in `model_dump_dir`.  A hard coded the model used during development can be found below.  We will load this model, and then pass our validation set to it, and generate ROC curves. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b1ad2-a650-42ab-8113-49d76a7fd0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NOTE: You will need to put your latest model file here\n",
    "model_path = \"//path/to/model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24e7ea-e2e1-4417-bc6d-f06a4dfbfd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from polygeist.CNN import PDNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d2c89e-5aad-423a-a17b-aee5417d2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network and ship it to the graphics card, use the output model file\n",
    "model = PDNet()\n",
    "model.apply_state(model_path)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2f125e-2ccd-4a37-a74f-ac1011ba75b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will implement transforms to get the images into the network like in the training\n",
    "transform_for_input = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(299),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bef0b8f-a017-48d0-9bd3-894ff1d743f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through and classify all the images\n",
    "results = {}\n",
    "for s in [\"Controls\", \"PD\"]:\n",
    "    for file in glob(f\"{training_dump_path}/val/{s}/*.jpg\"):\n",
    "        with Image.open(file) as im:\n",
    "            f = transform_for_input(im).to(device)\n",
    "            with torch.set_grad_enabled(False):\n",
    "                results[file] = model(f.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f459b-bafc-4d2e-9ad7-aaaac5247954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First - Patch level discrimination\n",
    "outputs = []\n",
    "labels = []\n",
    "for key, value in results.items():\n",
    "    outputs.append(float(value))\n",
    "    labels.append(1.0 if \"C\" in key else 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974255f8-fdb3-4425-bd95-bc68f7c3c9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(labels, 1.0 - np.array(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d427c353-5c4d-4174-bd36-1c814fa801b4",
   "metadata": {},
   "source": [
    "## Per ROI Classification Accuracy\n",
    "\n",
    "Here we evaluate the per patch accuracy for classifying PD and Control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ceaa1b-e9d0-457b-9ad0-a280e48f4033",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"PD vs Control\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Alarm Rate\", fontsize=18)\n",
    "plt.ylabel(\"Hit Rate\", fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2ea714-b235-4337-a17f-e956eae5daa9",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this workbook we have started with slides, digitised samples of brain tissue from either control brains or those with PD pathology, and we have:\n",
    "   - Identified likely regions of interest using a spectral estimation technique to find DAB stain.\n",
    "   - Segmented those ROIs into a size that can be processed by ML algorithms.\n",
    "   - Partitioned those data into training and test sets.\n",
    "   - Trained a CNN (PDNet) to discriminate those ROIs\n",
    "   - Produced a validation ROC curve demonstrating performance on unseen ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ab5aa3-f7d1-4054-89fd-38abcf72fc49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
