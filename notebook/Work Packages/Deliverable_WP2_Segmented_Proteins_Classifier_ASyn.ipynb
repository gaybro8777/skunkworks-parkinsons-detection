{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df89ed-0470-4d42-bf17-ddf5b5d1f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d828ff-001a-4acb-8b57-3a32c9668bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "import fnmatch\n",
    "import random\n",
    "import time\n",
    "from glob import glob\n",
    "\n",
    "import imageio as io\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from polygeist.label import process_files_and_folders\n",
    "from polygeist.slidecore.slide import AperioSlide as Slide\n",
    "from polygeist.slidecore.slide import SpectralSlideGenerator\n",
    "from polygeist.training import train_model\n",
    "from polygeist.utils import (\n",
    "    SegmentationFilesDirectoryHandler,\n",
    "    get_case_and_slide,\n",
    "    load_filenames_and_generate_conditions,\n",
    ")\n",
    "from polygeist.validation import validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f7eed4-2ff1-44e7-bb3f-eee2d7241556",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This workbook produces a classifier that can detect a-syn from colourimetric segmentations.  It will either segment the SVS slide files, using the spectral decomposition technique described in WP1, or generate some random noise images if simulated data is checked.  The dumped files will then be sorted into train and test, and processed using PDNet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d883445f-3a7b-4eb2-ae19-e81045e5192b",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "The configuration here specifies where the SVS files are, a 'working root' where we will dump the images, and protein specific configurations.  If you are using synthethic data, the case_files will not be used, but we will use the case identifiers for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487fc69-6d65-4d93-83ca-1e0db7cb9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # This is where the SVS slides are stored\n",
    "    \"svs_data_location\": \"/home/brad/localnas/\",\n",
    "    # This is the directory where all our segmentations, model files and sets will be stored\n",
    "    \"working_root\": \"/run/media/brad/ScratchM2/asyn_512_wp2_run/\",  # \"/run/media/brad/ScratchM2/test_dump/\",#\n",
    "    # These are our case filenames, which we shall parse to ensure case level segmenting in training and test\n",
    "    \"case_files\": \"Data/filenames/asyn_files.txt\",\n",
    "    # Segmentation Specific Information\n",
    "    # This is the stride over which we will look (the window size)\n",
    "    \"stride\": 512,\n",
    "    # The PUK set contains ID-INDEX_Protein in the filename, so here we specify 17_A (DMNoV, slide 17)\n",
    "    \"index\": \"17_A-syn\",\n",
    "    # This is the threshold under which a DAB activation will be considered noise\n",
    "    \"raw_threshold\": -0.3,\n",
    "    # This is the amount of pixels (as a percentage) per region that have to be activated to define a ROI\n",
    "    \"class_threshold\": 0.00125,\n",
    "    # PDNET Configuration\n",
    "    \"batch_size\": 16,  # Adjust for memory constraints (may affect results)\n",
    "    \"num_epochs\": 500,  # Adjust for time available for training (may affect results)\n",
    "    # Toggle this parameter if you have genearted simulated slides.\n",
    "    \"simulated_data\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3541c-ac49-4406-a6fb-950fa114f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dump path for our model training run, model checkpoints will be saved here\n",
    "model_dump_dir = f\"{config['working_root']}/model_dump/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca5f54-215f-4ecd-b0f1-7960d58b87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the cases and our conditions for each\n",
    "case_conditions = load_filenames_and_generate_conditions(config[\"case_files\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de0772-51c7-44bd-9bee-ada23bd77a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly split conditions\n",
    "def split_cases_into_train_and_test(case_cond, condition):\n",
    "    train = []\n",
    "    test = []\n",
    "    switch = False\n",
    "    for key, value in case_cond.items():\n",
    "        if condition not in value:\n",
    "            continue\n",
    "        if switch:\n",
    "            train.append(key)\n",
    "        else:\n",
    "            test.append(key)\n",
    "        switch = not switch\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861070e-d8f8-49da-a252-2a2532206c25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Splitting Cases (Brains) into Training and Test\n",
    "\n",
    "Here we split the cases into groups for training and test, then if we are using simulated data, we will just simulate a bunch of random regions to pass to PDNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69de48-e289-4f35-b183-5dc9fac7d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train, pd_test = split_cases_into_train_and_test(case_conditions, \"PD\")\n",
    "con_train, con_test = split_cases_into_train_and_test(case_conditions, \"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57723e9-a1d0-48d5-9c5b-afc3a5bf9e04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if config[\"simulated_data\"]:\n",
    "    for subset in [pd_train, pd_test, con_train, con_test]:\n",
    "        for case in subset:\n",
    "            for n in np.arange(0, np.random.randint(10)):\n",
    "                filename = f\"{config['working_root']}/{case}-17_A-syn.svs{n}.jpg\"\n",
    "                print(f\"Generating and writing random image to {filename}\")\n",
    "                SpectralSlideGenerator(\n",
    "                    width=config[\"stride\"], height=config[\"stride\"], filename=filename\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eafe3f-5758-46b4-b623-2f172b826bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is belt and braces to ensure that we do not have a set intersection.\n",
    "if len(list(set(pd_train) & set(pd_test))) > 0:\n",
    "    print(\"There is an overlap between training and test images for PD\")\n",
    "if len(list(set(con_train) & set(con_test))) > 0:\n",
    "    print(\"There is an overlap between training and test images for Control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e669b901-75ce-4d4c-9cca-e2aec3c8ee67",
   "metadata": {},
   "source": [
    "# Segmentation\n",
    "\n",
    "The 'process_files_and_folders' routine is the main segmentaion procedure.  This will load and spectrally decompose our data into DAB channels, and then dump each ROI identified into the working directory.  We skip this for simulated data, as all our regions are already random data.  Should you wish to run this, see the WP1 workbook.\n",
    "\n",
    "This routine will report the status of each segmented slide.  It can take many hours to complete if every slide is being processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf0c94-85bc-4ffe-9325-2bccd3ae9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not config[\"simulated_data\"]:\n",
    "    process_files_and_folders(\n",
    "        # The input data folder, this is where the SVS files are located\n",
    "        config[\"svs_data_location\"],\n",
    "        # Where we would like to dump the segmentations, and json files\n",
    "        config[\"working_root\"],\n",
    "        # This is the stride over which we will look (the window size)\n",
    "        stride=config[\"stride\"],\n",
    "        # This is the threshold under which a DAB activation will be considered noise\n",
    "        raw_threshold=config[\"raw_threshold\"],\n",
    "        # This is the amount of pixels (as a percentage) per region that have to be activated to define a ROI\n",
    "        class_threshold=config[\"class_threshold\"],\n",
    "        # Do not output full res density images\n",
    "        output_density=False,\n",
    "        # Output json metadata & density information\n",
    "        output_json=True,\n",
    "        # Skip outputting whole JPEGs\n",
    "        skip_jpeg=True,\n",
    "        # Automatically remove the slide background (note this is specialised to PUK Brain Slide Protocol)\n",
    "        auto_remove_background=True,\n",
    "        # Include all slides, but only A-beta stain.\n",
    "        include_only_index=config[\"index\"],\n",
    "        # Output each ROI as a JPEG for CNN training (and obs)\n",
    "        output_segmentation=True,\n",
    "        # Please provide print feedback on processing\n",
    "        verbose=True,\n",
    "        # Toggle this flag if you are using synthetic data.  Note, your root should be full of synthetic jpegs\n",
    "        synthetic=config[\"simulated\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59f048d-6477-4a20-b254-738118f14680",
   "metadata": {},
   "source": [
    "## Splits\n",
    "\n",
    "The file handler will split the files into a 'train' and 'val' directory and further into group folders for the torch Dataset handlers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a661750-c322-4c47-b4c5-0a7759d8710b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_handler = SegmentationFilesDirectoryHandler(config[\"working_root\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40910a13-6fdb-41f7-ac4f-997f4d543ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_handler.make_train_and_validation_folders_for_conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7e618-55f2-481e-9933-41429fc4c487",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_handler.split_and_copy_root_data_to_train_and_validation(\n",
    "    case_filter_for_train=pd_train,\n",
    "    condition=\"PD\",\n",
    "    training=True,\n",
    "    slide_index_filter=[17],\n",
    ")\n",
    "files_handler.split_and_copy_root_data_to_train_and_validation(\n",
    "    case_filter_for_train=con_train,\n",
    "    condition=\"Controls\",\n",
    "    training=True,\n",
    "    slide_index_filter=[17],\n",
    ")\n",
    "files_handler.split_and_copy_root_data_to_train_and_validation(\n",
    "    case_filter_for_train=pd_test,\n",
    "    condition=\"PD\",\n",
    "    training=False,\n",
    "    slide_index_filter=[17],\n",
    ")\n",
    "files_handler.split_and_copy_root_data_to_train_and_validation(\n",
    "    case_filter_for_train=con_test,\n",
    "    condition=\"Controls\",\n",
    "    training=False,\n",
    "    slide_index_filter=[17],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5149a8e8-55eb-40e6-ae73-a7f6a9ee7a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dump_path = config[\"working_root\"] + \"/partitioned_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318f2c5-e92e-43db-9f94-502621b918ac",
   "metadata": {},
   "source": [
    "## Balancing\n",
    "\n",
    "This routine will sample the raw slides to find more control regions, should the number of control samples be low.  This is only relevant in the case of real data, so it is skipped for synthetic data.  The segmentation procedure is very good at not producing ROIs for the control group, and therefore there would be a data inbalance during training, so by randomly sampling we can preserve the data balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08c9f58-0f27-4637-8635-aa8af89e791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to balance the training and test sets, to prevent overfitting, we do this by randomly sampling new\n",
    "# regions from the control set images until the number of control squares is the same of the test squares.\n",
    "# !! NOTE !! This only works when set A > set B.  If set B was larger, we would have to get more examples from set A\n",
    "# where a random sample would not work.\n",
    "l_control = len(glob(f\"{training_dump_path}/train/Controls/*.jpg\"))\n",
    "l_test = len(glob(f\"{training_dump_path}/train/PD/*.jpg\"))\n",
    "\n",
    "if not config[\"simulated_data\"]:\n",
    "    # This function will traverse our raw slides directory and gather candidate files that match our\n",
    "    # criteria of slide 17, and in the valid set\n",
    "    def get_valid_control_file_list(valid_cases):\n",
    "        matches = []\n",
    "        for root, dirnames, filenames in os.walk(config[\"svs_data_location\"]):\n",
    "            for filename in fnmatch.filter(filenames, \"*.svs\"):\n",
    "                case, slide = get_case_and_slide(filename)\n",
    "                if case not in case_conditions:\n",
    "                    continue\n",
    "                if case in valid_cases and slide == 17:\n",
    "                    matches.append(os.path.join(root, filename))\n",
    "        return matches\n",
    "\n",
    "    control_file_list = get_valid_control_file_list(con_train)\n",
    "    # Continue while set is unbalanced ~100\n",
    "    control_injection_index = 0\n",
    "    while l_test > l_control:\n",
    "        # Randomise the list\n",
    "        random.shuffle(control_file_list)\n",
    "\n",
    "        # Sample the top of the list '0th' element is the top which\n",
    "        # has just been shuffled\n",
    "        slide = Slide(control_file_list[0]).get_slide_with_pixel_resolution_in_microns(\n",
    "            2.0\n",
    "        )\n",
    "        filename = os.path.basename(control_file_list[0])\n",
    "\n",
    "        yy, xx, _ = slide.shape\n",
    "\n",
    "        # Create a densities array to store the local densities\n",
    "        x_pass = int(np.ceil(xx / 512))\n",
    "        y_pass = int(np.ceil(yy / 512))\n",
    "\n",
    "        # Make sure we are well within the slide tissue\n",
    "        for x, y in zip(\n",
    "            np.random.randint(4, x_pass - 4, 25), np.random.randint(4, y_pass - 4, 25)\n",
    "        ):\n",
    "            im = slide[\n",
    "                (y * 512) : (y * 512) + 512, (x * 512) : (x * 512) + 512, :\n",
    "            ].copy()\n",
    "            io.imwrite(\n",
    "                f\"{training_dump_path}/train/Controls/CI_{filename}_{control_injection_index}.jpg\",\n",
    "                im,\n",
    "            )\n",
    "            control_injection_index += 1\n",
    "\n",
    "        # Recount\n",
    "        l_control = len(glob(f\"{training_dump_path}/train/Controls/*.jpg\"))\n",
    "        l_test = len(glob(f\"{training_dump_path}/train/PD/*.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7770e3f3-79de-44ed-8588-423e66d23127",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We now pass all our parameters such as where the images are dumped, and the model directory where we will put our epoch checkpoint files, to the train_model routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c95ca-0321-4ef7-8c5d-823bcc948080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't inject into the validation set, that is kept clean for validation of the colourimetric segmentation.\n",
    "\n",
    "# Start a timer\n",
    "start_time = time.time()\n",
    "\n",
    "latest_model_name = train_model(\n",
    "    training_dump_path,\n",
    "    model_dump_dir,\n",
    "    config[\"batch_size\"],\n",
    "    config[\"num_epochs\"],\n",
    "    strict=False,\n",
    ")\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "print(f\"Training complete in {time_elapsed // 60}m {time_elapsed % 60}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2156e-b925-4460-9809-0651ccc90f08",
   "metadata": {},
   "source": [
    "## Validation\n",
    "\n",
    "Here we load up the last checkpoint file, I have left it hard code, so make sure you change the name to the model file that you have generated. \n",
    "\n",
    "Then we will pass our new model filename and the training path to our validation routine, which will run a pass on the 'val' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8514950a-f20d-422c-82cf-26562c9ab355",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I am renaming the model name here as I am running this later, but you comment this out otherwise.\n",
    "latest_model_name = f\"PDNET_checkpoint_70_11_05_12\"\n",
    "# Now we can run validation, on slide and case level\n",
    "# latest_model_name will have our last model, or it maybe specified manually.\n",
    "# E.g. model_file = f\"{model_dump_dir}/PDNET_checkpoint_490_16_18_48\"\n",
    "model_file = f\"{model_dump_dir}/{latest_model_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0c67a9-1d8c-48da-85ca-10dff5112b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_and_labels = validate(model_file, training_dump_path, config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14da4b3c-d50a-4875-88ce-4ad2615242f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = np.hstack(output_data_and_labels[\"outputs\"])\n",
    "labels = np.hstack(output_data_and_labels[\"labels\"])\n",
    "\n",
    "matched = outputs[labels == 1.0]\n",
    "non_matched = outputs[labels == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66450c2e-b4a7-4b83-9d59-0dda6eec7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude the excluded cases (label 2 is the 'EXCLUDE' folder), if there is no EXCLUDE folder, this does nothing.\n",
    "outputs = outputs[labels < 2]\n",
    "labels = labels[labels < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf94c8-10ce-4577-ad24-fb4f5208d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(labels, outputs, drop_intermediate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20668433-7152-4ccd-bf30-9faff076520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"Asyn Pathology vs Control\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Alarm Rate\", fontsize=18)\n",
    "plt.ylabel(\"Hit Rate\", fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac592391-510e-461b-ab12-4e5125279547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an index for the threshold\n",
    "th = 70\n",
    "print(f\"Threshold = {thresholds[th]}, TP : {tpr[th]}, FP {fpr[th]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f918ddb-2b71-4b6f-82e3-60a69ee008e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the confusion matrix\n",
    "t = thresholds[th]\n",
    "N_0 = len(outputs[labels == 0])\n",
    "N_1 = len(outputs[labels == 1])\n",
    "conf = [\n",
    "    (np.sum(outputs[labels == 0] < t) / N_0, np.sum(outputs[labels == 0] >= t) / N_0),\n",
    "    (np.sum(outputs[labels == 1] < t) / N_1, np.sum(outputs[labels == 1] >= t) / N_1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06194d4a-7ab1-43eb-8880-5cc5154cc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "print(\"\".ljust(10), \"Control\".ljust(10), \"Path\".ljust(10))\n",
    "print(\"Control\".rjust(10), f\"{conf[0][0]:.4f}\".ljust(10), f\"{conf[0][1]:.4f}\".ljust(10))\n",
    "print(\"Path\".rjust(10), f\"{conf[1][0]:.4f}\".ljust(10), f\"{conf[1][1]:.4f}\".ljust(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c138b885-e525-48b1-911a-5c6373dfef47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
