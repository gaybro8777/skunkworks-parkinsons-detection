{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec857b-3f48-4f7b-bfaf-3a79d6244f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../..')\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from polygeist.utils import load_filenames_and_generate_conditions\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from skimage.transform import resize\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dcda9d-1493-42ab-87ff-0017640ffd2e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This workbook uses the density maps produced in WP2, and the staging information provided by ICL, to produce a classifier.  This classifier can perform binary and multi class classfication of pathology based on those denisty maps.\n",
    "\n",
    "The steps here are to:\n",
    "\n",
    "- Define a data handler that can abstract the density maps and classifications into tensors and labels for training and validation\n",
    "- Define a small CNN that will be used during training, and any data transforms to be applied\n",
    "- Perform training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbb95b-a164-4914-a9a7-156b3a27f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BraakDataset(Dataset):\n",
    "    def __init__(self, cases, root, spreadsheet='/home/brad/repos/ParkinsonsSyntheticStaining/Data/'\n",
    "                                                'cases_for_staging_asyn_c319_1_sg.xlsx',\n",
    "                 transform=None, slide_size=32, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "                 reduce_classes=False, reduce_type='binary'):\n",
    "        # Load Braak Stages\n",
    "        braak_staging = pd.read_excel(spreadsheet, index_col=0)\n",
    "\n",
    "        slide_labels = ['01_A', '02_A', '04_A', '15_A', '16_A', '17_A']\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self.device = device\n",
    "\n",
    "        for case in tqdm(cases, desc=\"Loading Cases\"):\n",
    "            density_map_array = np.zeros((slide_size, slide_size, len(slide_labels)))\n",
    "            if case not in braak_staging.index:\n",
    "                continue\n",
    "\n",
    "            stage = braak_staging.loc[case]['STAGE']\n",
    "            if reduce_classes:\n",
    "                if 'binary' in reduce_type:\n",
    "                    stage = stage > 0\n",
    "                else:\n",
    "                    stage = 0 if stage < 1 else (2 if stage == 6 else 1)\n",
    "\n",
    "            files = glob(f\"{root}/{case}*.json\")\n",
    "            for i, label in enumerate(slide_labels):\n",
    "                find_file = next((item for item in [x if label in x else None for x in files] if item is not None), [])\n",
    "                if find_file:\n",
    "                    with open(find_file, 'r') as fp:\n",
    "                        density_map = np.array(json.load(fp)['densities'])\n",
    "                        resized_map = resize(density_map, (slide_size, slide_size))\n",
    "                        density_map_array[:, :, i] = np.array(resized_map)\n",
    "\n",
    "            density_map_array = np.moveaxis(density_map_array, -1, 0)\n",
    "            self.data.append(density_map_array)\n",
    "            self.targets.append(stage)\n",
    "\n",
    "        self.data = torch.Tensor(self.data).to(self.device)\n",
    "        self.targets = torch.Tensor(self.targets).to(self.device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713824ca-a549-48c0-ae50-6294ef42dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The simple CNN to be used during training\n",
    "class SeqBlock(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Sequential(nn.Conv2d(cin, cin, 4), nn.Conv2d(cin, cin, 3), nn.Conv2d(cin, cout, 2))\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "    def forward(self, x):\n",
    "        return self.pool(F.relu(self.c1(x)))\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, binary=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = SeqBlock(6, 6)\n",
    "        self.drop1 = nn.Dropout(p=0.15)\n",
    "        self.conv2 = SeqBlock(6, 5)\n",
    "        self.conv3 = SeqBlock(5, 2)\n",
    "        self.linear = nn.Linear(8, 3 if not binary else 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.conv3(self.conv2(x))\n",
    "        x = x.flatten(1)\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6265ee-0459-4ac6-bf27-6edf72e6ac9f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Loading and Processing the Dataset\n",
    "\n",
    "Here we get the dataset and shuffle the case, then we sample the top n'th percent and use those during training.  We create our dataset objects for use in training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6c251-b44f-43ab-870d-3871c7023b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_conditions = load_filenames_and_generate_conditions(\"Data/filenames/asyn_files.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466dc0d-ead9-4daf-a204-8c8032ad81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = .50\n",
    "pd_set = []\n",
    "con_set = []\n",
    "for k, v in case_conditions.items():\n",
    "    if 'PD' in v:\n",
    "        pd_set.append(k)\n",
    "    else:\n",
    "        con_set.append(k)\n",
    "random.shuffle(pd_set)\n",
    "random.shuffle(con_set)\n",
    "sample = int(len(con_set) * prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d892113-ba74-47f8-8f48-09dc21e63835",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_train = pd_set[0:sample]\n",
    "pd_test = pd_set[sample+1:]\n",
    "con_train = con_set[0:sample]\n",
    "con_test = con_set[sample+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048852e3-f31e-43ad-bf1c-ef030145940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Should these parameters be in a dict at the start of the notebook as in the previous ones?\n",
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "slide_size = 64\n",
    "reduce_classes = True\n",
    "\n",
    "train_dataset = BraakDataset(pd_train+ con_train, \"/run/media/brad/ScratchM2/asyn_rerun_128\", transform=None,\n",
    "                             slide_size=slide_size, reduce_classes=reduce_classes, reduce_type='binary')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=25, shuffle=True)\n",
    "\n",
    "test_dataset = BraakDataset(pd_test + con_test, \"/run/media/brad/ScratchM2/asyn_rerun_128\", transform=None,\n",
    "                            slide_size=slide_size, reduce_classes=reduce_classes, reduce_type='binary')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=25, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6e44d-c38d-410e-a3f1-4f46d0e37dbd",
   "metadata": {},
   "source": [
    "# Training and Validation\n",
    "\n",
    "Here we run training with a high epoch count (this is due to a small network and tiny dataset).  Then we load the state dictionary saved during the training step, and evaluate our test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a194b-11e8-4805-a6c1-34eb327fc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(dev)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "t_loss_history = []\n",
    "t_correct_history = []\n",
    "v_correct_history = []\n",
    "v_loss_history = []\n",
    "model = model.train()\n",
    "\n",
    "num_epochs = 10000\n",
    "best_val = 0\n",
    "\n",
    "for epoch in tqdm(np.arange(num_epochs)):\n",
    "    sum_loss = 0.0\n",
    "    correct = 0\n",
    "    run_count = 0\n",
    "    for i, dl in enumerate(train_dataloader, 0):\n",
    "        data, labels = dl\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        classification = torch.sum(outputs * torch.arange(0, outputs.shape[1]).to(dev), 1)\n",
    "        loss = criterion(classification, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item()\n",
    "        correct += torch.sum(torch.round(classification) == labels) / len(labels)\n",
    "        t_loss_history.append(sum_loss)\n",
    "        t_correct_history.append(correct)\n",
    "        run_count += 1\n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"Train -> Epoch {epoch} / {num_epochs} | Loss : {sum_loss}, Correct : {correct / run_count}\")\n",
    "    model.eval()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0\n",
    "    run_count = 0\n",
    "    for i, dl in enumerate(test_dataloader, 0):\n",
    "        data, labels = dl\n",
    "        outputs = model(data)\n",
    "        classification = torch.sum(outputs * torch.arange(0, outputs.shape[1]).to(dev), 1)\n",
    "        loss = criterion(classification, labels)\n",
    "        sum_loss += loss.item()\n",
    "        correct += torch.sum(torch.round(classification) == labels) / len(labels)\n",
    "        v_loss_history.append(sum_loss)\n",
    "        v_correct_history.append(correct)\n",
    "        run_count += 1\n",
    "    if epoch % 250 == 0:\n",
    "        print(f\"Validation -> Epoch {epoch} / {num_epochs} | Loss : {sum_loss}, Correct : {correct / run_count}\")\n",
    "    if correct / run_count > best_val and num_epochs > 3:\n",
    "        torch.save(model.state_dict(), f\"mmclass_{datetime.now().strftime('%d_%m_%Y__%H_%M_%S')}_.pth\")\n",
    "        best_val = correct / run_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f629ad-e565-4a58-aa7e-b2c22bb0a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Would it make more sense to have a branch in the notebook for \"train xor load\" based on a config option?\n",
    "model.load_state_dict(torch.load('mmclass_23_02_2023__16_09_21_.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c73f0-06bf-43af-bd2d-0a8674bc336f",
   "metadata": {},
   "source": [
    "`Train -> Epoch 6900 / 10000 | Loss : 0.04006555293199199, Correct : 0.9942857623100281`\n",
    "`Validation -> Epoch 6900 / 10000 | Loss : 0.9827206871559611, Correct : 0.8920000195503235`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a5b27-2792-4b98-8b4a-de9d8be7a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_results = []\n",
    "running_labels = []\n",
    "for i, dl in enumerate(test_dataloader, 0):\n",
    "    data, labels = dl\n",
    "    outputs = model(data)\n",
    "    classification = torch.sum(outputs * torch.arange(0, outputs.shape[1]).to(dev), 1)\n",
    "    running_results.append(torch.round(classification).tolist())\n",
    "    running_labels.append(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046189c-9738-4550-9db0-845117e42c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_labels = np.hstack(running_labels)\n",
    "conf_results = np.hstack(running_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898dc268-cf65-481d-acfe-120c7b1acb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(conf_labels, conf_results > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2bfbea-8b5a-4e84-b2cc-d01ac7d83f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3124f43-a46c-4e0a-8da6-81b5381d4d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat[0, :] / np.sum(confusion_mat[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8756c213-926d-4b5e-a0a6-52a2b1bc530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat[1, :] / np.sum(confusion_mat[1, :])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
