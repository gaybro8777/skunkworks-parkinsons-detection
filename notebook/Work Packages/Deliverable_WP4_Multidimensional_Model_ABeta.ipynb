{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec857b-3f48-4f7b-bfaf-3a79d6244f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from skimage.transform import resize\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from polygeist.utils import load_filenames_and_generate_conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f15ee-9c84-4b52-bd87-5af1533f284d",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This workbook uses the density maps produced in WP2, and is designed to use all density maps from a case to perform case level classification.  This classifier can perform binary and multi class classfication of pathology based on those denisty maps.\n",
    "\n",
    "The steps here are to:\n",
    "\n",
    "- Define a data handler that can abstract the density maps and classifications into tensors and labels for training and validation\n",
    "- Define a small CNN that will be used during training, and any data transforms to be applied\n",
    "- Perform training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbb95b-a164-4914-a9a7-156b3a27f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ABetaDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cases,\n",
    "        root,\n",
    "        transform=None,\n",
    "        slide_size=32,\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "        case_conditions=None,\n",
    "    ):\n",
    "\n",
    "        slide_labels = [\"1_A\", \"10_A\", \"14_A\", \"15_A\", \"09_A\", \"05_A\"]\n",
    "\n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        self.device = device\n",
    "\n",
    "        for case in tqdm(cases, desc=\"Loading Cases\"):\n",
    "            density_map_array = np.zeros((slide_size, slide_size, len(slide_labels)))\n",
    "\n",
    "            if case_conditions:\n",
    "                condition = case_conditions[case]\n",
    "                class_index = 1 if \"AD\" in condition else 0\n",
    "            else:\n",
    "                # TODO: Should this be case_index or class_index?\n",
    "                case_index = 0 if \"C\" in case else 1\n",
    "\n",
    "            files = glob(f\"{root}/{case}*.json\")\n",
    "            for i, label in enumerate(slide_labels):\n",
    "                find_file = next(\n",
    "                    (\n",
    "                        item\n",
    "                        for item in [x if label in x else None for x in files]\n",
    "                        if item is not None\n",
    "                    ),\n",
    "                    [],\n",
    "                )\n",
    "                if find_file:\n",
    "                    with open(find_file, \"r\") as fp:\n",
    "                        density_map = np.array(json.load(fp)[\"densities\"])\n",
    "                        resized_map = resize(density_map, (slide_size, slide_size))\n",
    "                        density_map_array[:, :, i] = np.array(resized_map)\n",
    "\n",
    "            density_map_array = np.moveaxis(density_map_array, -1, 0)\n",
    "            self.data.append(density_map_array)\n",
    "            self.targets.append(class_index)\n",
    "\n",
    "        self.data = torch.Tensor(self.data).to(self.device)\n",
    "        self.targets = torch.Tensor(self.targets).to(self.device)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e068e974-f589-43dc-b5ea-ee2e80340427",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeqBlock(nn.Module):\n",
    "    def __init__(self, cin, cout):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Sequential(\n",
    "            nn.Conv2d(cin, cin, 4), nn.Conv2d(cin, cin, 3), nn.Conv2d(cin, cout, 2)\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pool(F.relu(self.c1(x)))\n",
    "\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, binary=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = SeqBlock(6, 6)\n",
    "        self.drop1 = nn.Dropout(p=0.15)\n",
    "        self.conv2 = SeqBlock(6, 5)\n",
    "        self.conv3 = SeqBlock(5, 2)\n",
    "        self.linear = nn.Linear(8, 3 if not binary else 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.conv3(self.conv2(x))\n",
    "        x = x.flatten(1)\n",
    "        x = self.linear(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6c251-b44f-43ab-870d-3871c7023b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_conditions = load_filenames_and_generate_conditions(\n",
    "    \"Data/filenames/abeta_files.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d466dc0d-ead9-4daf-a204-8c8032ad81cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_set = []\n",
    "con_set = []\n",
    "for k, v in case_conditions.items():\n",
    "    if \"AD\" in v:\n",
    "        ad_set.append(k)\n",
    "    else:\n",
    "        con_set.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc5c147-2e1a-479c-9fda-bc3a40b2533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = 0.60\n",
    "random.shuffle(ad_set)\n",
    "random.shuffle(con_set)\n",
    "sample = int(len(ad_set) * prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d892113-ba74-47f8-8f48-09dc21e63835",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_train = ad_set[0:sample]\n",
    "ad_test = ad_set[sample + 1 :]\n",
    "con_train = con_set[0:sample]\n",
    "con_test = con_set[sample + 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700359ca-50e3-4f5c-b8bd-ecff345c9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048852e3-f31e-43ad-bf1c-ef030145940e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "slide_size = 64\n",
    "reduce_classes = True\n",
    "\n",
    "train_dataset = ABetaDataset(\n",
    "    ad_train + con_train,\n",
    "    \"/run/media/brad/ScratchM2/ABeta_label_dump_64\",\n",
    "    transform=None,\n",
    "    slide_size=slide_size,\n",
    "    case_conditions=case_conditions,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=25, shuffle=True)\n",
    "\n",
    "test_dataset = ABetaDataset(\n",
    "    ad_test + con_test,\n",
    "    \"/run/media/brad/ScratchM2/ABeta_label_dump_64\",\n",
    "    transform=None,\n",
    "    slide_size=slide_size,\n",
    "    case_conditions=case_conditions,\n",
    ")\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a194b-11e8-4805-a6c1-34eb327fc0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(dev)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "t_loss_history = []\n",
    "t_correct_history = []\n",
    "v_correct_history = []\n",
    "v_loss_history = []\n",
    "model = model.train()\n",
    "\n",
    "num_epochs = 10000\n",
    "best_val = 0\n",
    "\n",
    "for epoch in tqdm(np.arange(num_epochs)):\n",
    "    sum_loss = 0.0\n",
    "    correct = 0\n",
    "    run_count = 0\n",
    "    for i, dl in enumerate(train_dataloader, 0):\n",
    "        data, labels = dl\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        classification = torch.sum(\n",
    "            outputs * torch.arange(0, outputs.shape[1]).to(dev), 1\n",
    "        )\n",
    "        loss = criterion(classification, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sum_loss += loss.item()\n",
    "        correct += torch.sum(torch.round(classification) == labels) / len(labels)\n",
    "        t_loss_history.append(sum_loss)\n",
    "        t_correct_history.append(correct)\n",
    "        run_count += 1\n",
    "    if epoch % 250 == 0:\n",
    "        print(\n",
    "            f\"Train -> Epoch {epoch} / {num_epochs} | Loss : {sum_loss}, Correct : {correct / run_count}\"\n",
    "        )\n",
    "    model.eval()\n",
    "    sum_loss = 0.0\n",
    "    correct = 0\n",
    "    run_count = 0\n",
    "    for i, dl in enumerate(test_dataloader, 0):\n",
    "        data, labels = dl\n",
    "        outputs = model(data)\n",
    "        classification = torch.sum(\n",
    "            outputs * torch.arange(0, outputs.shape[1]).to(dev), 1\n",
    "        )\n",
    "        loss = criterion(classification, labels)\n",
    "        sum_loss += loss.item()\n",
    "        correct += torch.sum(torch.round(classification) == labels) / len(labels)\n",
    "        v_loss_history.append(sum_loss)\n",
    "        v_correct_history.append(correct)\n",
    "        run_count += 1\n",
    "    if epoch % 250 == 0:\n",
    "        print(\n",
    "            f\"Validation -> Epoch {epoch} / {num_epochs} | Loss : {sum_loss}, Correct : {correct / run_count}\"\n",
    "        )\n",
    "    if correct / run_count > best_val and num_epochs > 3:\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            f\"mmclass_{datetime.now().strftime('%d_%m_%Y__%H_%M_%S')}_.pth\",\n",
    "        )\n",
    "        best_val = correct / run_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f629ad-e565-4a58-aa7e-b2c22bb0a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"mmclass_02_03_2023__15_28_05_.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716c73f0-06bf-43af-bd2d-0a8674bc336f",
   "metadata": {},
   "source": [
    "Train -> Epoch 9750 / 10000 | Loss : 0.049242664128541946, Correct : 0.9545454978942871\n",
    "Validation -> Epoch 9750 / 10000 | Loss : 0.5373633205890656, Correct : 0.7199999690055847"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8a5b27-2792-4b98-8b4a-de9d8be7a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "running_results = []\n",
    "running_labels = []\n",
    "for i, dl in enumerate(test_dataloader, 0):\n",
    "    data, labels = dl\n",
    "    outputs = model(data)\n",
    "    classification = torch.sum(outputs * torch.arange(0, outputs.shape[1]).to(dev), 1)\n",
    "    running_results.append(torch.round(classification).tolist())\n",
    "    running_labels.append(labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046189c-9738-4550-9db0-845117e42c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_labels = np.hstack(running_labels)\n",
    "conf_results = np.hstack(running_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898dc268-cf65-481d-acfe-120c7b1acb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat = confusion_matrix(conf_labels, conf_results > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16da6c5a-1de2-4c0c-a080-8b7c9377d98f",
   "metadata": {},
   "source": [
    "# ⚠️ Health Warning ⚠️\n",
    "We have way too many cases to train and test the model rigorously.  This is a simple test to see if there was some differentiation in the groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f23cbf-bb78-430e-9b14-e7f6e21fe3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13384558-9351-451d-b1e5-9c2184cc52f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: As in WP3 notebook re confusion matrix rendering\n",
    "print(confusion_mat[0, :] / np.sum(confusion_mat[0, :]))\n",
    "print(confusion_mat[1, :] / np.sum(confusion_mat[1, :]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
