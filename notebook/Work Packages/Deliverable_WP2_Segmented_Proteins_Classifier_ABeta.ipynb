{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86df89ed-0470-4d42-bf17-ddf5b5d1f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d828ff-001a-4acb-8b57-3a32c9668bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"../..\")\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "from polygeist.label import process_files_and_folders\n",
    "from polygeist.training import train_model\n",
    "from polygeist.utils import (\n",
    "    SegmentationFilesDirectoryHandler,\n",
    "    load_filenames_and_generate_conditions,\n",
    ")\n",
    "from polygeist.validation import validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab65991-2b17-44af-a2d9-06f7d72902c9",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This workbook will process the ABeta SVS slide files, producing regions of interest (ROIs) as jpegs for classification using the PDNet CNN.  Because Abeta pathology exists most prevalently in our AD+PD group, we will be primarily using that group as our test condition.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Search through the data directory with `polygeist.label`, which will process our SVS files and produce ROIs for classification.\n",
    "- Segment those ROIs into training and test sets for use with PDNet\n",
    "- Run the PDNet training routine on those training images\n",
    "- Validate the model by loading it and running it on the validate dataset\n",
    "- Use `sklearn.metrics` to evaluate our model, and plot an ROC function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487fc69-6d65-4d93-83ca-1e0db7cb9c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    # This is where the SVS slides are stored\n",
    "    \"svs_data_location\": \"/home/brad/localnas/\",\n",
    "    # This is the directory where all our segmentations, model files and sets will be stored\n",
    "    \"working_root\": \"/run/media/brad/ScratchM2/ABeta_label_dump_64/\",\n",
    "    # These are our case filenames, which we shall parse to ensure case level segmenting in training and test\n",
    "    \"case_files\": \"Data/filenames/abeta_files.txt\",\n",
    "    # Segmentation Specific Information\n",
    "    # This is the stride over which we will look (the window size)\n",
    "    \"stride\": 64,\n",
    "    # The PUK set contains ID-INDEX_Protein in the filename, so here we specify A-beta (only use Abeta stained slides)\n",
    "    \"index\": \"A-beta\",\n",
    "    # This is the threshold under which a DAB activation will be considered noise\n",
    "    \"raw_threshold\": -0.1,\n",
    "    # This is the amount of pixels (as a percentage) per region that have to be activated to define a ROI\n",
    "    \"class_threshold\": 0.01,\n",
    "    # PDNET Configuration\n",
    "    \"batch_size\": 32,  # Adjust for memory constraints (may affect results)\n",
    "    \"num_epochs\": 500,  # Adjust for time available for training (may affect results)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c3541c-ac49-4406-a6fb-950fa114f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our dump path for our model training run, model checkpoints will be saved here\n",
    "model_dump_dir = f\"{config['working_root']}/model_dump/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ca5f54-215f-4ecd-b0f1-7960d58b87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the cases and our conditions for each\n",
    "case_conditions = load_filenames_and_generate_conditions(config[\"case_files\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31de0772-51c7-44bd-9bee-ada23bd77a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniformly split by condition\n",
    "def split_cases_into_train_and_test(case_cond, condition):\n",
    "    train = []\n",
    "    test = []\n",
    "    switch = False\n",
    "    for key, value in case_cond.items():\n",
    "        if condition not in value:\n",
    "            continue\n",
    "        if switch:\n",
    "            train.append(key)\n",
    "        else:\n",
    "            test.append(key)\n",
    "        switch = not switch\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d69de48-e289-4f35-b183-5dc9fac7d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_train, ad_test = split_cases_into_train_and_test(case_conditions, \"AD\")\n",
    "con_train, con_test = split_cases_into_train_and_test(case_conditions, \"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2dfc73-0d4f-4e45-842e-8dce9b42a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_cases(case_list, exclude):\n",
    "    return [x for x in case_list if exclude not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137bf362-2804-46ec-82d2-db34f768f540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have an erroneous case, case PD303/PD0303, so we will remove it from our lists\n",
    "ad_train = exclude_cases(ad_train, \"303\")\n",
    "ad_test = exclude_cases(ad_test, \"303\")\n",
    "con_train = exclude_cases(con_train, \"303\")\n",
    "con_test = exclude_cases(con_test, \"303\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7f05a-499d-44af-913c-30e2381d2d53",
   "metadata": {},
   "source": [
    "# Spectral Estimation & Segmentation\n",
    "\n",
    "This process utility (from `polygeist.label`) takes a list of protein specific parameters and performs the spectral estimation technique to produce estimates of the DAB staining.  It then segments ROIs with specific parameters and dumps them to disk as jpegs for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bf0c94-85bc-4ffe-9325-2bccd3ae9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_files_and_folders(\n",
    "    # The input data folder, this is where the SVS files are located\n",
    "    config[\"svs_data_location\"],\n",
    "    # Where we would like to dump the segmentations, and json files\n",
    "    config[\"working_root\"],\n",
    "    # This is the stride over which we will look (the window size)\n",
    "    stride=config[\"stride\"],\n",
    "    # This is the threshold under which a DAB activation will be considered noise\n",
    "    raw_threshold=config[\"raw_threshold\"],\n",
    "    # This is the amount of pixels (as a percentage) per region that have to be activated to define a ROI\n",
    "    class_threshold=config[\"class_threshold\"],\n",
    "    # Do not output full res density images\n",
    "    output_density=False,\n",
    "    # Output json metadata & density information\n",
    "    output_json=True,\n",
    "    # Skip outputting whole JPEGs\n",
    "    skip_jpeg=True,\n",
    "    # Automatically remove the slide background (note this is specialised to PUK Brain Slide Protocol)\n",
    "    auto_remove_background=True,\n",
    "    # Include all slides, but only A-beta stain.\n",
    "    include_only_index=\"A-beta\",\n",
    "    # Output each ROI as a JPEG for CNN training (and obs)\n",
    "    output_segmentation=True,\n",
    "    # Please provide print feedback on processing\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d09c16-b440-442e-8b1d-05bf170809e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a clean copy of the data for performance, repeatability and safety.\n",
    "training_dump_path = f\"{config['working_root']}/partitioned_data\"\n",
    "files_handler = SegmentationFilesDirectoryHandler(config[\"working_root\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f262b816-fab1-4464-9b40-40b7d1a3a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_handler.make_train_and_validation_folders_for_conditions(\n",
    "    conditions=[\"Controls\", \"AD\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55360e31-5fd1-4a8b-94b5-e75f47a845bb",
   "metadata": {},
   "source": [
    "# Sorting Data into Training and Test\n",
    "\n",
    "Here we sort all regions into either training or test sets, we balance by the N images in the AD condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d400e419-6d98-44cc-a4b4-6018e1b113d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_handler.split_and_copy_root_data_to_train_and_validation(\n",
    "    case_filter_for_train=ad_train,\n",
    "    condition=\"AD\",\n",
    "    training=True,\n",
    "    slide_index_filter=[10],\n",
    ")\n",
    "files_handler.split_and_copy_root_data_to_train_and_validation(\n",
    "    case_filter_for_train=con_train,\n",
    "    condition=\"Controls\",\n",
    "    training=True,\n",
    "    slide_index_filter=[10],\n",
    ")\n",
    "files_handler.split_and_copy_root_data_to_train_and_validation(\n",
    "    case_filter_for_train=ad_test,\n",
    "    condition=\"AD\",\n",
    "    training=False,\n",
    "    slide_index_filter=[10],\n",
    ")\n",
    "files_handler.split_and_copy_root_data_to_train_and_validation(\n",
    "    case_filter_for_train=con_test,\n",
    "    condition=\"Controls\",\n",
    "    training=False,\n",
    "    slide_index_filter=[10],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e15c3e-43f4-4f6a-a5aa-d92b611b0a4f",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "The data layout is passed to the train_model utility to produce us a PDNet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3c95ca-0321-4ef7-8c5d-823bcc948080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't inject into the validation set, that is kept clean for validation of the colourimetric segmentation.\n",
    "\n",
    "# Start a timer\n",
    "start_time = time.time()\n",
    "\n",
    "latest_model_name = train_model(\n",
    "    training_dump_path,\n",
    "    model_dump_dir,\n",
    "    config[\"batch_size\"],\n",
    "    config[\"num_epochs\"],\n",
    "    strict=False,\n",
    ")\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "print(f\"Training complete in {time_elapsed // 60}m {time_elapsed % 60}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df710d2c",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "latest_model_name = f\"PDNET_checkpoint_10_18_51_38\"\n",
    "# Now we can run validation, on slide and case level\n",
    "# latest_model_name will have our last model, or it maybe specified manually.\n",
    "# E.g. model_file = f\"{model_dump_dir}/PDNET_checkpoint_490_16_18_48\"\n",
    "model_file = f\"{model_dump_dir}/{latest_model_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb4bd08-8636-4041-baf3-d177be87b3ec",
   "metadata": {},
   "source": [
    "# Model Validation\n",
    "\n",
    "The data layout is passed to the validation utility to produce us validation scores.  Here we load up the last checkpoint file, I have left it hard code, so make sure you change the name to the model file that you have generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86bf8b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "output_data_and_labels = validate(model_file, training_dump_path, config[\"batch_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07181231",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "outputs = np.hstack(output_data_and_labels[\"outputs\"])\n",
    "labels = np.hstack(output_data_and_labels[\"labels\"])\n",
    "\n",
    "matched = outputs[labels == 1.0]\n",
    "non_matched = outputs[labels == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e28120",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Exclude the excluded cases (label 2 is the 'EXCLUDE' folder), if there is no EXCLUDE folder, this does nothing.\n",
    "outputs = outputs[labels < 2]\n",
    "labels = labels[labels < 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf94c8-10ce-4577-ad24-fb4f5208d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(labels, outputs, drop_intermediate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20668433-7152-4ccd-bf30-9faff076520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=\"ABeta Pathology vs Control\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"False Alarm Rate\", fontsize=18)\n",
    "plt.ylabel(\"Hit Rate\", fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "plt.xticks(fontsize=18)\n",
    "# plt.title(\"512um Patch Level Discrmination between Taupathology and Control Tau Segmentation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac592391-510e-461b-ab12-4e5125279547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an index for the threshold\n",
    "th = 10\n",
    "print(f\"Threshold = {thresholds[th]}, TP : {tpr[th]}, FP {fpr[th]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f918ddb-2b71-4b6f-82e3-60a69ee008e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the confusion matrix\n",
    "t = thresholds[th]\n",
    "N_0 = len(outputs[labels == 0])\n",
    "N_1 = len(outputs[labels == 1])\n",
    "conf = [\n",
    "    (np.sum(outputs[labels == 0] < t) / N_0, np.sum(outputs[labels == 0] >= t) / N_0),\n",
    "    (np.sum(outputs[labels == 1] < t) / N_1, np.sum(outputs[labels == 1] >= t) / N_1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06194d4a-7ab1-43eb-8880-5cc5154cc47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "print(\"\".ljust(10), \"Control\".ljust(10), \"Path\".ljust(10))\n",
    "print(\"Control\".rjust(10), f\"{conf[0][0]:.4f}\".ljust(10), f\"{conf[0][1]:.4f}\".ljust(10))\n",
    "print(\"Path\".rjust(10), f\"{conf[1][0]:.4f}\".ljust(10), f\"{conf[1][1]:.4f}\".ljust(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e3f2f-b300-46b7-bd26-61e9d24a7d45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
